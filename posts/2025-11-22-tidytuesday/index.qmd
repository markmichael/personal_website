
---
title: "TidyTuesday 2025-11-18 - Sherlock Holmes"
date: "2025-11-22"
categories: ["data science", "tidytuesday", "rstats"]
---

## The Complete Works of Sherlock Holmes

TidyTuesday is a weekly data visualization challenge. The details for this week can be found [here](https://github.com/rfordatascience/tidytuesday/tree/main/data/2025/2025-11-18).

### Introduction


```{r setup1, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(gt)
```

```{r setup2, include=FALSE}
data <- tidytuesdayR::tt_load("2025-11-18")[[1]]

```


Analysing literature with data science and numerical methods is a fun journey. A few years ago I read [Nabokov's favorite word is mauve](https://www.goodreads.com/book/show/30753786-nabokov-s-favorite-word-is-mauve) by Ben Blatt. The methods for quantifying literature are both interesting and compelling, and I'm interested in seeing if there are similar patterns here. Sidenote: that book was described to me as "the most NPR book ever," and I love that.

My first step is to explore samples of the data to see if it's usable as is.
<br> <br>

```{r}

books <- data |>
  distinct(book) |>
  mutate(publish_order = row_number())



text_sample <- data[2000:2100,]

text_sample |>
  gt() |>
  tab_header(title = "Sample of Text") |>
  opt_stylize(style = 6, color = "gray") |>
  tab_options(container.height = "300px", container.overflow.y = "scroll")

```

<br> <br>
The most remarkable thing from the sample is that each observation represents a literal line in the novel. The lines can be concatenated so that each observation is a paragraph. This will be particularly useful in understanding dialog because it is typical in literature to start a new paragraph each time a speaker changes. The NAs between lines mean we have a good boundary between paragraphs.

Additionally, I cross-referenced the novel order with the [Canon of Sherlock Holmes](https://en.wikipedia.org/wiki/Canon_of_Sherlock_Holmes) in order to understand the order in which these works were published. I think it could be interesting to see if there are changes that occur between earlier works and later works.
<br> <br>

```{r}

paragraphs <- data |>
  mutate(paragraph = cumsum(is.na(text))) |>
  filter(!is.na(text)) |>
  group_by(book,paragraph) |>
  summarize(text = paste(text, collapse = " "))

paragraphs_sample <- paragraphs[2000:2010,]

paragraphs_sample |>
  gt() |>
  tab_header(title = "Sample of Paragraphs") |>
  opt_stylize(style = 6, color = "gray") |>
  tab_options(container.height = "300px", container.overflow.y = "scroll")

```

<br> <br>

### Exclamation points 

The analysis from Blatt that I'm interested in replicating is his exclamation point analysis.

F. Scott Fitzgerald and Earnest Hemingway were known for their very resvered use of exclamation points. Fizgerald describes it as "Laughing at your own joke." We can use this data to determine how well Arthur Conan Doyle follows this rule.
<br> <br>

```{r}

exclamation_points <- paragraphs |>
  mutate(exclamation_points = str_count(text, "!"))

exclamation_point_sample <- exclamation_points |>
  filter(exclamation_points > 0) |>
  ungroup() |>
  slice_sample(n = 5)

### showing some example lines as a sanity check to make sure our code is working

exclamation_point_sample |>
  gt() |>
  tab_header(title = "Sample of Lines with Exclamation Points") |>
  opt_stylize(style = 6, color = "gray") |>
  tab_options(container.height = "300px", container.overflow.y = "scroll")

exclamation_points_summary <- exclamation_points |>
  group_by(book) |>
  summarize(exclamation_points = sum(exclamation_points)) |>
  left_join(books, by = "book") |>
  arrange(publish_order)

### actually counting exclamation points

exclamation_points_summary |>
  gt() |>
  tab_header(title = "Exclamation Points per Book") |>
  tab_options(container.height = "500px", container.overflow.y = "scroll") |>
  cols_label(
    book = "Book",
    exclamation_points = "Exclamation Points",
    publish_order = "Publish Order"
  ) |>
  fmt_number(columns = exclamation_points, decimals = 0)
```

<br> <br>
Noteably the 4 highest counts are full novels, while the rest are short stories. It makes sense that they are higher, but they will skew the rest of the analysis.

In graphical form:
<br> <br>

```{r}

novels <- c("A Study In Scarlet",
            "The Sign of the Four",
            "The Hound of the Baskervilles",
            "The Valley Of Fear" )

exclamation_points_summary |>
  filter(!(book %in% novels)) |>
  ggplot(aes(x = publish_order, y = exclamation_points)) +
  geom_point() +
  labs(
    x = "Publish Order",
    y = "Exclamation Points",
    title = "Exclamation Points per Book"
  )
```

<br> <br>
Graphically, it does not look like Doyle's use of exclamation points changed over time. We can verify this mathematically.
<br> <br>

```{r}

exclamation_points_summary |>
  filter(!(book %in% novels)) |>
  lm(exclamation_points ~ publish_order, data = _) |>
  summary()

```

<br> <br>

Sometimes there's not anything particularly exciting with a type of analysis; sometimes just seeing that he remains consistent throughout is career as an awesome insight in itself.

### Edit 2025-11-24

I received an excellent suggestion around normalization: <br>

<blockquote class="bluesky-embed" data-bluesky-uri="at://did:plc:777eanw7giyjd76dwatpch3d/app.bsky.feed.post/3m6ffv3zjsc24" data-bluesky-cid="bafyreigbi7aq7jcevml26qhusk5wwnnfc6nredwekdtnc3uevhvulotriq" data-bluesky-embed-color-mode="system"><p lang="en">Excellent!! It would be cool to see exclamation marks divided by word count! Since longer works will have more by nature! How many exclamation marks can I get in this post?! ðŸ˜‚</p>&mdash; Libby Heeren (<a href="https://bsky.app/profile/did:plc:777eanw7giyjd76dwatpch3d?ref_src=embed">@libbyheeren.bsky.social</a>) <a href="https://bsky.app/profile/did:plc:777eanw7giyjd76dwatpch3d/post/3m6ffv3zjsc24?ref_src=embed">November 24, 2025 at 10:02 AM</a></blockquote><script async src="https://embed.bsky.app/static/embed.js" charset="utf-8"></script>

<br>

Indeed, just because I only included the short stories in the previous visualisation does not mean that all of Doyle's short stories are same length, and therefore, the absolute count comparision might not necessarily be fair. That said, there is one more method for normalization that is also worth considering. Hemingway would argue that most exclamation points should be replaced with periods. In addition to the above suggestion, I will also normalize by the dividing the sum of terminal periods and exclamation points. This one is a bit more tricky. I don't want to count "Dr." or "1." as sentences. To correct for this, I will count periods immediately after a word that has at least one lowercase vowel.

```{r}

exclamation_points_normalization <- paragraphs |>
  mutate(exclamation_points = str_count(text, "!"),
         words = lengths(str_split(text, " ")),
         terminal_periods = str_count(text, "\\b\\w*[aeiou]\\w*\\.")
         ) |>
  slice_sample(n=2000:2010)

```

