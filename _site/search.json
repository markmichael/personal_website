[
  {
    "objectID": "posts/2026-02-03-edible-plants/index.html",
    "href": "posts/2026-02-03-edible-plants/index.html",
    "title": "TidyTuesday 2026-02-03 Edible Plants",
    "section": "",
    "text": "TidyTuesday is a weekly data visualization challenge. The details for this week can be found here.\n\nIntroduction\nThis week we‚Äôre looking at edible plants. I‚Äôm interested in understanding the relationship between sun and water requirements and the caloric content of the plant. Generally I would expect that if a plant needs more energy from the sun, it‚Äôs because it is storing more sugars in it‚Äôs fruit.\n\nknitr::opts_chunk$set(echo = TRUE)\nlibrary(tidyverse)\n\nI use the tidytuesdayR package to load the data the first time. Then I store it locally in a csv.\n\nedible_plants &lt;- tidytuesdayR::tt_load(\"2026-02-03\")[[1]]\nwrite_csv(edible_plants, \"edible_plants.csv\")\n\n\nedible_plants &lt;- read_csv(\"edible_plants.csv\", show_col_types = FALSE) |&gt;\n  mutate(sunlight = tolower(sunlight) |&gt; str_replace_all(\"/ \", \"/\") |&gt; factor())\n\nedible_plants$water &lt;- factor(edible_plants$water, levels= c(\"Very Low\", \"Low\", \"Medium\", \"High\", \"Very High\"))\n\n\n\nExploring columns of interest\nLet‚Äôs look at the values in the sunlight, water, and energy columns.\n\nedible_plants |&gt;\n  select(sunlight, water, energy, nutrients) |&gt;\n  glimpse()\n\nRows: 140\nColumns: 4\n$ sunlight  &lt;fct&gt; full sun, full sun, full sun, full sun/partial shade/full sh‚Ä¶\n$ water     &lt;fct&gt; Very High, Medium, Medium, Very Low, High, Medium, Medium, M‚Ä¶\n$ energy    &lt;dbl&gt; 80, 27, 27, 88, 25, 34, 35, 31, 50, NA, NA, NA, NA, NA, NA, ‚Ä¶\n$ nutrients &lt;chr&gt; \"Medium\", \"Medium\", \"Medium\", \"Medium\", \"High\", \"High\", \"Hig‚Ä¶\n\n\nIt looks like there are a lot of NA values for energy, but we can probably use nutrients as a another output measurement. Also our inputs are going to be factors not numbers. This will impact the types of analyses we can use.\nLet‚Äôs start with a basic visualization for the non-NA energy content plants.\n\nedible_plants |&gt;\n  filter(!is.na(energy)) |&gt;\n  ggplot(aes(x = water, y = energy)) +\n  facet_grid(rows = vars(sunlight)) +\n  geom_boxplot() +\n  theme_light()\n\n\n\n\n\n\n\n\nCouple notable things here:\n\nIt looks the ability to tolerate shade is correlated with a lower caloric content.\nA surprising outlier, broad beans, or fava beans, can grow in full shade, with low water, and have the highest caloric content.\n\nIf we had more energy values, we could formalize our observations with a two-way ANOVA.\nInstead, let‚Äôs look at the nutrients dependent variable. Because this is categorical as well, we‚Äôll use an ordinal logistic regression. It‚Äôs important to note that nutrients isn‚Äôt a replacement for the energy variable. It describes the nutrient needs for the plant to grow. So a reasonable hypothesis is that if you have plenty of sun and water, you can don‚Äôt need to rely on extra nutrients in the soil.\n\nedible_plants &lt;- edible_plants |&gt;\n  filter(nutrients %in% c(\"Low\", \"Medium\", \"High\"))\nedible_plants$nutrients &lt;- factor(edible_plants$nutrients, levels = c(\"Low\", \"Medium\", \"High\"))\nm &lt;-MASS::polr(nutrients ~ sunlight + water, data = edible_plants, Hess = TRUE)\nsummary(m)\n\nCall:\nMASS::polr(formula = nutrients ~ sunlight + water, data = edible_plants, \n    Hess = TRUE)\n\nCoefficients:\n                                             Value Std. Error t value\nsunlightfull sun/partial shade             -0.8533     0.3881  -2.199\nsunlightfull sun/partial shade/full shade -28.1526     1.4209 -19.813\nsunlightpartial shade                      -0.8243     0.8149  -1.012\nwaterLow                                  -31.1595     0.7538 -41.336\nwaterMedium                               -28.1510     0.4607 -61.104\nwaterHigh                                 -26.7838     0.5692 -47.058\nwaterVery High                            -28.1526     1.4209 -19.813\n\nIntercepts:\n            Value    Std. Error t value \nLow|Medium  -29.3183   0.4370   -67.0915\nMedium|High -26.9870   0.4527   -59.6119\n\nResidual Deviance: 238.4313 \nAIC: 256.4313 \n(2 observations deleted due to missingness)\n\n\nThe reference values here are ‚ÄúFull Sun‚Äù and ‚ÄúVery Low [water]‚Äù. What we see here is the opposite. The ability to tolerate full shade suggests a lower nutrient requirement. Similarly any amount of water beyond ‚ÄúVery Low‚Äù suggests a lower nutrient requirement. Maybe the better way to think about it is that sunlight or derivative sugar molecules are used with the nutrients in biochemical reactions.\nWe can represent this visually here:\n\nedible_plants |&gt;\n  filter(nutrients %in% c(\"Low\", \"Medium\", \"High\")) |&gt;\n  filter(!is.na(water)) |&gt;\n  ggplot(aes(x = nutrients)) +\n  facet_grid(rows = vars(sunlight), cols = vars(water)) +\n  geom_bar() +\n  theme_light() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))"
  },
  {
    "objectID": "posts/2025-11-22-tidytuesday/index.html",
    "href": "posts/2025-11-22-tidytuesday/index.html",
    "title": "TidyTuesday 2025-11-18 - Sherlock Holmes",
    "section": "",
    "text": "TidyTuesday is a weekly data visualization challenge. The details for this week can be found here.\n\n\nAnalysing literature with data science and numerical methods is a fun journey. A few years ago I read Nabokov‚Äôs favorite word is mauve by Ben Blatt. The methods for quantifying literature are both interesting and compelling, and I‚Äôm interested in seeing if there are similar patterns here. Sidenote: that book was described to me as ‚Äúthe most NPR book ever,‚Äù and I love that.\nMy first step is to explore samples of the data to see if it‚Äôs usable as is.  \n\nbooks &lt;- data |&gt;\n  distinct(book) |&gt;\n  mutate(publish_order = row_number())\n\n\n\ntext_sample &lt;- data[2000:2100,]\n\ntext_sample |&gt;\n  gt() |&gt;\n  tab_header(title = \"Sample of Text\") |&gt;\n  opt_stylize(style = 6, color = \"gray\") |&gt;\n  tab_options(container.height = \"300px\", container.overflow.y = \"scroll\")\n\n\n\n\n\n\n\nSample of Text\n\n\nbook\ntext\nline_num\n\n\n\n\nA Study In Scarlet\n\"'You had best tell me all about it now,' I said. 'Half-confidences\n2000\n\n\nA Study In Scarlet\nare worse than none. Besides, you do not know how much we know of\n2001\n\n\nA Study In Scarlet\nit.'\n2002\n\n\nA Study In Scarlet\nNA\n2003\n\n\nA Study In Scarlet\n\"'On your head be it, Alice!' cried her mother; and then, turning to\n2004\n\n\nA Study In Scarlet\nme, 'I will tell you all, sir. Do not imagine that my agitation on\n2005\n\n\nA Study In Scarlet\nbehalf of my son arises from any fear lest he should have had a hand\n2006\n\n\nA Study In Scarlet\nin this terrible affair. He is utterly innocent of it. My dread is,\n2007\n\n\nA Study In Scarlet\nhowever, that in your eyes and in the eyes of others he may appear to\n2008\n\n\nA Study In Scarlet\nbe compromised. That however is surely impossible. His high\n2009\n\n\nA Study In Scarlet\ncharacter, his profession, his antecedents would all forbid it.'\n2010\n\n\nA Study In Scarlet\nNA\n2011\n\n\nA Study In Scarlet\n\"'Your best way is to make a clean breast of the facts,' I answered.\n2012\n\n\nA Study In Scarlet\n'Depend upon it, if your son is innocent he will be none the worse.'\n2013\n\n\nA Study In Scarlet\nNA\n2014\n\n\nA Study In Scarlet\n\"'Perhaps, Alice, you had better leave us together,' she said, and\n2015\n\n\nA Study In Scarlet\nher daughter withdrew. 'Now, sir,' she continued, 'I had no intention\n2016\n\n\nA Study In Scarlet\nof telling you all this, but since my poor daughter has disclosed it\n2017\n\n\nA Study In Scarlet\nI have no alternative. Having once decided to speak, I will tell you\n2018\n\n\nA Study In Scarlet\nall without omitting any particular.'\n2019\n\n\nA Study In Scarlet\nNA\n2020\n\n\nA Study In Scarlet\n\"'It is your wisest course,' said I.\n2021\n\n\nA Study In Scarlet\nNA\n2022\n\n\nA Study In Scarlet\n\"'Mr. Drebber has been with us nearly three weeks. He and his\n2023\n\n\nA Study In Scarlet\nsecretary, Mr. Stangerson, had been travelling on the Continent. I\n2024\n\n\nA Study In Scarlet\nnoticed a \"Copenhagen\" label upon each of their trunks, showing that\n2025\n\n\nA Study In Scarlet\nthat had been their last stopping place. Stangerson was a quiet\n2026\n\n\nA Study In Scarlet\nreserved man, but his employer, I am sorry to say, was far otherwise.\n2027\n\n\nA Study In Scarlet\nHe was coarse in his habits and brutish in his ways. The very night\n2028\n\n\nA Study In Scarlet\nof his arrival he became very much the worse for drink, and, indeed,\n2029\n\n\nA Study In Scarlet\nafter twelve o'clock in the day he could hardly ever be said to be\n2030\n\n\nA Study In Scarlet\nsober. His manners towards the maid-servants were disgustingly free\n2031\n\n\nA Study In Scarlet\nand familiar. Worst of all, he speedily assumed the same attitude\n2032\n\n\nA Study In Scarlet\ntowards my daughter, Alice, and spoke to her more than once in a way\n2033\n\n\nA Study In Scarlet\nwhich, fortunately, she is too innocent to understand. On one\n2034\n\n\nA Study In Scarlet\noccasion he actually seized her in his arms and embraced her--an\n2035\n\n\nA Study In Scarlet\noutrage which caused his own secretary to reproach him for his\n2036\n\n\nA Study In Scarlet\nunmanly conduct.'\n2037\n\n\nA Study In Scarlet\nNA\n2038\n\n\nA Study In Scarlet\n\"'But why did you stand all this,' I asked. 'I suppose that you can\n2039\n\n\nA Study In Scarlet\nget rid of your boarders when you wish.'\n2040\n\n\nA Study In Scarlet\nNA\n2041\n\n\nA Study In Scarlet\n\"Mrs. Charpentier blushed at my pertinent question. 'Would to God\n2042\n\n\nA Study In Scarlet\nthat I had given him notice on the very day that he came,' she said.\n2043\n\n\nA Study In Scarlet\n'But it was a sore temptation. They were paying a pound a day\n2044\n\n\nA Study In Scarlet\neach--fourteen pounds a week, and this is the slack season. I am a\n2045\n\n\nA Study In Scarlet\nwidow, and my boy in the Navy has cost me much. I grudged to lose the\n2046\n\n\nA Study In Scarlet\nmoney. I acted for the best. This last was too much, however, and I\n2047\n\n\nA Study In Scarlet\ngave him notice to leave on account of it. That was the reason of his\n2048\n\n\nA Study In Scarlet\ngoing.'\n2049\n\n\nA Study In Scarlet\nNA\n2050\n\n\nA Study In Scarlet\n\"'Well?'\n2051\n\n\nA Study In Scarlet\nNA\n2052\n\n\nA Study In Scarlet\n\"'My heart grew light when I saw him drive away. My son is on leave\n2053\n\n\nA Study In Scarlet\njust now, but I did not tell him anything of all this, for his temper\n2054\n\n\nA Study In Scarlet\nis violent, and he is passionately fond of his sister. When I closed\n2055\n\n\nA Study In Scarlet\nthe door behind them a load seemed to be lifted from my mind. Alas,\n2056\n\n\nA Study In Scarlet\nin less than an hour there was a ring at the bell, and I learned that\n2057\n\n\nA Study In Scarlet\nMr. Drebber had returned. He was much excited, and evidently the\n2058\n\n\nA Study In Scarlet\nworse for drink. He forced his way into the room, where I was sitting\n2059\n\n\nA Study In Scarlet\nwith my daughter, and made some incoherent remark about having missed\n2060\n\n\nA Study In Scarlet\nhis train. He then turned to Alice, and before my very face, proposed\n2061\n\n\nA Study In Scarlet\nto her that she should fly with him. \"You are of age,\" he said, \"and\n2062\n\n\nA Study In Scarlet\nthere is no law to stop you. I have money enough and to spare. Never\n2063\n\n\nA Study In Scarlet\nmind the old girl here, but come along with me now straight away. You\n2064\n\n\nA Study In Scarlet\nshall live like a princess.\" Poor Alice was so frightened that she\n2065\n\n\nA Study In Scarlet\nshrunk away from him, but he caught her by the wrist and endeavoured\n2066\n\n\nA Study In Scarlet\nto draw her towards the door. I screamed, and at that moment my son\n2067\n\n\nA Study In Scarlet\nArthur came into the room. What happened then I do not know. I heard\n2068\n\n\nA Study In Scarlet\noaths and the confused sounds of a scuffle. I was too terrified to\n2069\n\n\nA Study In Scarlet\nraise my head. When I did look up I saw Arthur standing in the\n2070\n\n\nA Study In Scarlet\ndoorway laughing, with a stick in his hand. \"I don't think that fine\n2071\n\n\nA Study In Scarlet\nfellow will trouble us again,\" he said. \"I will just go after him and\n2072\n\n\nA Study In Scarlet\nsee what he does with himself.\" With those words he took his hat and\n2073\n\n\nA Study In Scarlet\nstarted off down the street. The next morning we heard of Mr.\n2074\n\n\nA Study In Scarlet\nDrebber's mysterious death.'\n2075\n\n\nA Study In Scarlet\nNA\n2076\n\n\nA Study In Scarlet\n\"This statement came from Mrs. Charpentier's lips with many gasps and\n2077\n\n\nA Study In Scarlet\npauses. At times she spoke so low that I could hardly catch the\n2078\n\n\nA Study In Scarlet\nwords. I made shorthand notes of all that she said, however, so that\n2079\n\n\nA Study In Scarlet\nthere should be no possibility of a mistake.\"\n2080\n\n\nA Study In Scarlet\nNA\n2081\n\n\nA Study In Scarlet\n\"It's quite exciting,\" said Sherlock Holmes, with a yawn. \"What\n2082\n\n\nA Study In Scarlet\nhappened next?\"\n2083\n\n\nA Study In Scarlet\nNA\n2084\n\n\nA Study In Scarlet\n\"When Mrs. Charpentier paused,\" the detective continued, \"I saw that\n2085\n\n\nA Study In Scarlet\nthe whole case hung upon one point. Fixing her with my eye in a way\n2086\n\n\nA Study In Scarlet\nwhich I always found effective with women, I asked her at what hour\n2087\n\n\nA Study In Scarlet\nher son returned.\n2088\n\n\nA Study In Scarlet\nNA\n2089\n\n\nA Study In Scarlet\n\"'I do not know,' she answered.\n2090\n\n\nA Study In Scarlet\nNA\n2091\n\n\nA Study In Scarlet\n\"'Not know?'\n2092\n\n\nA Study In Scarlet\nNA\n2093\n\n\nA Study In Scarlet\n\"'No; he has a latch-key, and he let himself in.'\n2094\n\n\nA Study In Scarlet\nNA\n2095\n\n\nA Study In Scarlet\n\"'After you went to bed?'\n2096\n\n\nA Study In Scarlet\nNA\n2097\n\n\nA Study In Scarlet\n\"'Yes.'\n2098\n\n\nA Study In Scarlet\nNA\n2099\n\n\nA Study In Scarlet\n\"'When did you go to bed?'\n2100\n\n\n\n\n\n\n\n  The most remarkable thing from the sample is that each observation represents a literal line in the novel. The lines can be concatenated so that each observation is a paragraph. This will be particularly useful in understanding dialog because it is typical in literature to start a new paragraph each time a speaker changes. The NAs between lines mean we have a good boundary between paragraphs.\nAdditionally, I cross-referenced the novel order with the Canon of Sherlock Holmes in order to understand the order in which these works were published. I think it could be interesting to see if there are changes that occur between earlier works and later works.  \n\nparagraphs &lt;- data |&gt;\n  mutate(paragraph = cumsum(is.na(text))) |&gt;\n  filter(!is.na(text)) |&gt;\n  group_by(book,paragraph) |&gt;\n  summarize(text = paste(text, collapse = \" \"))\n\n`summarise()` has grouped output by 'book'. You can override using the\n`.groups` argument.\n\nparagraphs_sample &lt;- paragraphs[2000:2010,]\n\nparagraphs_sample |&gt;\n  gt() |&gt;\n  tab_header(title = \"Sample of Paragraphs\") |&gt;\n  opt_stylize(style = 6, color = \"gray\") |&gt;\n  tab_options(container.height = \"300px\", container.overflow.y = \"scroll\")\n\n\n\n\n\n\n\nSample of Paragraphs\n\n\nparagraph\ntext\n\n\n\n\nThe Adventure of Charles Augustus Milverton\n\n\n7455\n\"What I say is true,\" Holmes answered. \"The money cannot be found. Surely it is better for you to take the substantial sum which I offer than to ruin this woman's career, which can profit you in no way?\"\n\n\n7456\n\"There you make a mistake, Mr. Holmes. An exposure would profit me indirectly to a considerable extent. I have eight or ten similar cases maturing. If it was circulated among them that I had made a severe example of the Lady Eva I should find all of them much more open to reason. You see my point?\"\n\n\n7457\nHolmes sprang from his chair.\n\n\n7458\n\"Get behind him, Watson! Don't let him out! Now, sir, let us see the contents of that note-book.\"\n\n\n7459\nMilverton had glided as quick as a rat to the side of the room, and stood with his back against the wall.\n\n\n7460\n\"Mr. Holmes, Mr. Holmes,\" he said, turning the front of his coat and exhibiting the butt of a large revolver, which projected from the inside pocket. \"I have been expecting you to do something original. This has been done so often, and what good has ever come from it? I assure you that I am armed to the teeth, and I am perfectly prepared to use my weapons, knowing that the law will support me. Besides, your supposition that I would bring the letters here in a note-book is entirely mistaken. I would do nothing so foolish. And now, gentlemen, I have one or two little interviews this evening, and it is a long drive to Hampstead.\" He stepped forward, took up his coat, laid his hand on his revolver, and turned to the door. I picked up a chair, but Holmes shook his head and I laid it down again. With bow, a smile, and a twinkle Milverton was out of the room, and a few moments after we heard the slam of the carriage door and the rattle of the wheels as he drove away.\n\n\n7461\nHolmes sat motionless by the fire, his hands buried deep in his trouser pockets, his chin sunk upon his breast, his eyes fixed upon the glowing embers. For half an hour he was silent and still. Then, with the gesture of a man who has taken his decision, he sprang to his feet and passed into his bedroom. A little later a rakish young workman with a goatee beard and a swagger lit his clay pipe at the lamp before descending into the street. \"I'll be back some time, Watson,\" said he, and vanished into the night. I understood that he had opened his campaign against Charles Augustus Milverton; but I little dreamed the strange shape which that campaign was destined to take.\n\n\n7462\nFor some days Holmes came and went at all hours in this attire, but beyond a remark that his time was spent at Hampstead, and that it was not wasted, I knew nothing of what he was doing. At last, however, on a wild, tempestuous evening, when the wind screamed and rattled against the windows, he returned from his last expedition, and having removed his disguise he sat before the fire and laughed heartily in his silent inward fashion.\n\n\n7463\n\"You would not call me a marrying man, Watson?\"\n\n\n7464\n\"No, indeed!\"\n\n\n7465\n\"You'll be interested to hear that I am engaged.\"\n\n\n\n\n\n\n\n \n\n\n\nThe analysis from Blatt that I‚Äôm interested in replicating is his exclamation point analysis.\nF. Scott Fitzgerald and Earnest Hemingway were known for their very resvered use of exclamation points. Fizgerald describes it as ‚ÄúLaughing at your own joke.‚Äù We can use this data to determine how well Arthur Conan Doyle follows this rule.  \n\nexclamation_points &lt;- paragraphs |&gt;\n  mutate(exclamation_points = str_count(text, \"!\"))\n\nexclamation_point_sample &lt;- exclamation_points |&gt;\n  filter(exclamation_points &gt; 0) |&gt;\n  ungroup() |&gt;\n  slice_sample(n = 5)\n\n### showing some example lines as a sanity check to make sure our code is working\n\nexclamation_point_sample |&gt;\n  gt() |&gt;\n  tab_header(title = \"Sample of Lines with Exclamation Points\") |&gt;\n  opt_stylize(style = 6, color = \"gray\") |&gt;\n  tab_options(container.height = \"300px\", container.overflow.y = \"scroll\")\n\n\n\n\n\n\n\nSample of Lines with Exclamation Points\n\n\nbook\nparagraph\ntext\nexclamation_points\n\n\n\n\nThe Adventure of the Noble Bachelor\n3585\n\"Without, however, the knowledge of pre-existing cases which serves me so well. There was a parallel instance in Aberdeen some years back, and something on very much the same lines at Munich the year after the Franco-Prussian War. It is one of these cases--but, hullo, here is Lestrade! Good-afternoon, Lestrade! You will find an extra tumbler upon the sideboard, and there are cigars in the box.\"\n2\n\n\nThe Adventure of the Devil's Foot\n13067\n\"It won't do, Watson!\" said he with a laugh. \"Let us walk along the cliffs together and search for flint arrows. We are more likely to find them than clues to this problem. To let the brain work without sufficient material is like racing an engine. It racks itself to pieces. The sea air, sunshine, and patience, Watson--all else will come.\n1\n\n\nThe Adventure of the Six Napoleons\n7640\n\"Splendid!\"\n1\n\n\nThe Man with the Twisted Lip\n2582\n\"Why,\" said my wife, pulling up her veil, \"it is Kate Whitney. How you startled me, Kate! I had not an idea who you were when you came in.\"\n1\n\n\nThe Sign of the Four\n1026\n\"It means murder,\" said he, stooping over the dead man. \"Ah, I expected it. Look here!\" He pointed to what looked like a long, dark thorn stuck in the skin just above the ear.\n1\n\n\n\n\n\n\nexclamation_points_summary &lt;- exclamation_points |&gt;\n  group_by(book) |&gt;\n  summarize(exclamation_points = sum(exclamation_points)) |&gt;\n  left_join(books, by = \"book\") |&gt;\n  arrange(publish_order)\n\n### actually counting exclamation points\n\nexclamation_points_summary |&gt;\n  gt() |&gt;\n  tab_header(title = \"Exclamation Points per Book\") |&gt;\n  tab_options(container.height = \"500px\", container.overflow.y = \"scroll\") |&gt;\n  cols_label(\n    book = \"Book\",\n    exclamation_points = \"Exclamation Points\",\n    publish_order = \"Publish Order\"\n  ) |&gt;\n  fmt_number(columns = exclamation_points, decimals = 0)\n\n\n\n\n\n\n\nExclamation Points per Book\n\n\nBook\nExclamation Points\nPublish Order\n\n\n\n\nA Study In Scarlet\n85\n1\n\n\nThe Sign of the Four\n127\n2\n\n\nA Scandal in Bohemia\n33\n3\n\n\nThe Red-Headed League\n16\n4\n\n\nA Case of Identity\n13\n5\n\n\nThe Boscombe Valley Mystery\n27\n6\n\n\nThe Five Orange Pips\n17\n7\n\n\nThe Man with the Twisted Lip\n32\n8\n\n\nThe Adventure of the Blue Carbuncle\n40\n9\n\n\nThe Adventure of the Speckled Band\n34\n10\n\n\nThe Adventure of the Engineer's Thumb\n31\n11\n\n\nThe Adventure of the Noble Bachelor\n19\n12\n\n\nThe Adventure of the Beryl Coronet\n43\n13\n\n\nThe Adventure of the Copper Beeches\n40\n14\n\n\nSilver Blaze\n36\n15\n\n\nThe Yellow Face\n12\n16\n\n\nThe Stock-Broker's Clerk\n29\n17\n\n\nThe \"Gloria Scott\"\n19\n18\n\n\nThe Musgrave Ritual\n7\n19\n\n\nThe Reigate Squires\n24\n20\n\n\nThe Crooked Man\n15\n21\n\n\nThe Resident Patient\n17\n22\n\n\nThe Greek Interpreter\n13\n23\n\n\nThe Naval Treaty\n34\n24\n\n\nThe Final Problem\n10\n25\n\n\nThe Adventure of the Empty House\n16\n26\n\n\nThe Adventure of the Norwood Builder\n24\n27\n\n\nThe Adventure of the Dancing Men\n17\n28\n\n\nThe Adventure of the Solitary Cyclist\n48\n29\n\n\nThe Adventure of the Priory School\n36\n30\n\n\nThe Adventure of Black Peter\n10\n31\n\n\nThe Adventure of Charles Augustus Milverton\n24\n32\n\n\nThe Adventure of the Six Napoleons\n17\n33\n\n\nThe Adventure of the Three Students\n23\n34\n\n\nThe Adventure of the Golden Pince-Nez\n36\n35\n\n\nThe Adventure of the Missing Three-Quarter\n34\n36\n\n\nThe Adventure of the Abbey Grange\n21\n37\n\n\nThe Adventure of the Second Stain\n49\n38\n\n\nThe Hound of the Baskervilles\n182\n39\n\n\nThe Valley Of Fear\n319\n40\n\n\nThe Adventure of Wisteria Lodge\n12\n41\n\n\nThe Adventure of the Cardboard Box\n20\n42\n\n\nThe Adventure of the Red Circle\n46\n43\n\n\nThe Adventure of the Bruce-Partington Plans\n43\n44\n\n\nThe Adventure of the Dying Detective\n45\n45\n\n\nThe Disappearance of Lady Frances Carfax\n40\n46\n\n\nThe Adventure of the Devil's Foot\n18\n47\n\n\nHis Last Bow\n23\n48\n\n\n\n\n\n\n\n  Noteably the 4 highest counts are full novels, while the rest are short stories. It makes sense that they are higher, but they will skew the rest of the analysis.\nIn graphical form:  \n\nnovels &lt;- c(\"A Study In Scarlet\",\n            \"The Sign of the Four\",\n            \"The Hound of the Baskervilles\",\n            \"The Valley Of Fear\" )\n\nexclamation_points_summary |&gt;\n  filter(!(book %in% novels)) |&gt;\n  ggplot(aes(x = publish_order, y = exclamation_points)) +\n  geom_point() +\n  labs(\n    x = \"Publish Order\",\n    y = \"Exclamation Points\",\n    title = \"Exclamation Points per Book\"\n  )\n\n\n\n\n\n\n\n\n  Graphically, it does not look like Doyle‚Äôs use of exclamation points changed over time. We can verify this mathematically.  \n\nexclamation_points_summary |&gt;\n  filter(!(book %in% novels)) |&gt;\n  lm(exclamation_points ~ publish_order, data = _) |&gt;\n  summary()\n\n\nCall:\nlm(formula = exclamation_points ~ publish_order, data = filter(exclamation_points_summary, \n    !(book %in% novels)))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-18.637 -10.027  -3.060   9.545  21.007 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    23.0608     3.8057   6.060 3.26e-07 ***\npublish_order   0.1356     0.1351   1.003    0.321    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.86 on 42 degrees of freedom\nMultiple R-squared:  0.02341,   Adjusted R-squared:  0.0001549 \nF-statistic: 1.007 on 1 and 42 DF,  p-value: 0.3214\n\n\n \nSometimes there‚Äôs not anything particularly exciting with a type of analysis; sometimes just seeing that he remains consistent throughout is career as an awesome insight in itself.\n\n\n\nI received an excellent suggestion around normalization: \n\n\nExcellent!! It would be cool to see exclamation marks divided by word count! Since longer works will have more by nature! How many exclamation marks can I get in this post?! üòÇ\n\n‚Äî Libby Heeren (@libbyheeren.bsky.social) November 24, 2025 at 10:02 AM\n\n\n\nIndeed, just because I only included the short stories in the previous visualisation does not mean that all of Doyle‚Äôs short stories are same length, and therefore, the absolute count comparision might not necessarily be fair. That said, there is one more method for normalization that is also worth considering. Hemingway would argue that most exclamation points should be replaced with periods. In addition to the above suggestion, I will also normalize by the dividing the sum of terminal periods and exclamation points. This one is a bit more tricky. I don‚Äôt want to count ‚ÄúDr.‚Äù or ‚Äú1.‚Äù as sentences. To correct for this, I will count periods immediately after a word that has at least one lowercase vowel.\n\nexclamation_points_normalization &lt;- paragraphs |&gt;\n  mutate(exclamation_points = str_count(text, \"!\"),\n         words = lengths(str_split(text, \" \")),\n         terminal_periods = str_count(text, \"\\\\b\\\\w*[aeiou]\\\\w*\\\\.\") #thanks chatGPT\n         )\nsample &lt;- exclamation_points_normalization |&gt;\n  ungroup() |&gt;\n  filter(exclamation_points &gt; 0) |&gt;\n  filter(terminal_periods &gt; 0) |&gt;\n  slice_sample(n=5)\n\nsample |&gt; #Trust but verify\n  gt() |&gt;\n  tab_header(title = \"Sample of Punctuation and Word Counts\") |&gt;\n  opt_stylize(style = 6, color = \"gray\") |&gt;\n  tab_options(container.height = \"300px\", container.overflow.y = \"scroll\") \n\n\n\n\n\n\n\nSample of Punctuation and Word Counts\n\n\nbook\nparagraph\ntext\nexclamation_points\nwords\nterminal_periods\n\n\n\n\nThe Adventure of the Solitary Cyclist\n6820\n\"Enough of this,\" said my friend, coldly. \"Drop that pistol! Watson, pick it up! Hold it to his head! Thank you. You, Carruthers, give me that revolver. We'll have no more violence. Come, hand it over!\"\n4\n36\n4\n\n\nThe Adventure of the Golden Pince-Nez\n8009\n\"Tobacco and my work, but now only tobacco,\" the old man exclaimed. \"Alas! what a fatal interruption! Who could have foreseen such a terrible catastrophe? So estimable a young man! I assure you that after a few months' training he was an admirable assistant. What do you think of the matter, Mr. Holmes?\"\n3\n53\n2\n\n\nThe Hound of the Baskervilles\n8905\n\"Excellent! This is a colleague, Watson, after our own heart. But the marks?\"\n1\n13\n1\n\n\nA Study In Scarlet\n50\n\"Indeed!\" I murmured.\n1\n3\n1\n\n\nThe Hound of the Baskervilles\n10056\n\"To see Sir Henry. Ah, here he is!\"\n1\n8\n1\n\n\n\n\n\n\n\n \n\nexclamation_points_normalization_summary &lt;- exclamation_points_normalization |&gt;\n  group_by(book) |&gt;\n  summarize(exclamation_points = sum(exclamation_points),\n            terminal_periods = sum(terminal_periods),\n            words = sum(words),\n            exclamation_point_per_word = exclamation_points / words,\n            exclamation_point_per_sentence_ending = exclamation_points / (terminal_periods + exclamation_points)) |&gt;\n  left_join(books, by = \"book\")\n\nmetrics &lt;- tibble(\n  Metric = c(\"Most Exclamations\", \"Least Exclamations\", \"Most Exclamations per Word\", \"Least Exclamations per Word\", \"Most Exclamations per Sentence Ending\", \"Least Exclamations per Sentence Ending\"),\n  book = c(\n    exclamation_points_normalization_summary$book[which.max(exclamation_points_normalization_summary$exclamation_points)],\n    exclamation_points_normalization_summary$book[which.min(exclamation_points_normalization_summary$exclamation_points)],\n    exclamation_points_normalization_summary$book[which.max(exclamation_points_normalization_summary$exclamation_point_per_word)],\n    exclamation_points_normalization_summary$book[which.min(exclamation_points_normalization_summary$exclamation_point_per_word)],\n    exclamation_points_normalization_summary$book[which.max(exclamation_points_normalization_summary$exclamation_point_per_sentence_ending)],\n    exclamation_points_normalization_summary$book[which.min(exclamation_points_normalization_summary$exclamation_point_per_sentence_ending)]\n  ),\n  Value = c(\n    max(exclamation_points_normalization_summary$exclamation_points),\n    min(exclamation_points_normalization_summary$exclamation_points),\n    max(exclamation_points_normalization_summary$exclamation_point_per_word),\n    min(exclamation_points_normalization_summary$exclamation_point_per_word),\n    max(exclamation_points_normalization_summary$exclamation_point_per_sentence_ending),\n    min(exclamation_points_normalization_summary$exclamation_point_per_sentence_ending)\n  )\n)\n\nmetrics |&gt;\n  gt() |&gt;\n  cols_label(\n    book = \"Book\",\n    Value = \"Value\"\n  ) |&gt;\n  fmt_number(columns = Value, decimals = 4) |&gt;\n  tab_header(title = \"Exclamation Point Usage Summary\")\n\n\n\n\n\n\n\nExclamation Point Usage Summary\n\n\nMetric\nBook\nValue\n\n\n\n\nMost Exclamations\nThe Valley Of Fear\n319.0000\n\n\nLeast Exclamations\nThe Musgrave Ritual\n7.0000\n\n\nMost Exclamations per Word\nThe Adventure of the Dying Detective\n0.0078\n\n\nLeast Exclamations per Word\nThe Musgrave Ritual\n0.0009\n\n\nMost Exclamations per Sentence Ending\nThe Adventure of the Solitary Cyclist\n0.1004\n\n\nLeast Exclamations per Sentence Ending\nThe Adventure of Wisteria Lodge\n0.0172\n\n\n\n\n\n\n\n \n\nexclamation_points_normalization_summary_for_plot &lt;- exclamation_points_normalization_summary |&gt;\n  pivot_longer(cols = c(exclamation_point_per_word, exclamation_point_per_sentence_ending)) |&gt;\n  mutate(name = recode(name,\n                         \"exclamation_point_per_word\" = \"Per Word\",\n                         \"exclamation_point_per_sentence_ending\" = \"Per Sentence Ending\"))\n\nexclamation_normalization_plot &lt;- exclamation_points_normalization_summary_for_plot |&gt;\n  ggplot(aes(x = publish_order, y = value)) +\n  facet_wrap(~name, scales = \"free_y\") +\n  geom_point() +\n  labs(x = \"Publish Order\",\n       y = NULL,\n       title = \"Normalized Exclamation Point Usage\")\nexclamation_normalization_plot"
  },
  {
    "objectID": "posts/2025-11-22-tidytuesday/index.html#the-complete-works-of-sherlock-holmes",
    "href": "posts/2025-11-22-tidytuesday/index.html#the-complete-works-of-sherlock-holmes",
    "title": "TidyTuesday 2025-11-18 - Sherlock Holmes",
    "section": "",
    "text": "TidyTuesday is a weekly data visualization challenge. The details for this week can be found here.\n\n\nAnalysing literature with data science and numerical methods is a fun journey. A few years ago I read Nabokov‚Äôs favorite word is mauve by Ben Blatt. The methods for quantifying literature are both interesting and compelling, and I‚Äôm interested in seeing if there are similar patterns here. Sidenote: that book was described to me as ‚Äúthe most NPR book ever,‚Äù and I love that.\nMy first step is to explore samples of the data to see if it‚Äôs usable as is.  \n\nbooks &lt;- data |&gt;\n  distinct(book) |&gt;\n  mutate(publish_order = row_number())\n\n\n\ntext_sample &lt;- data[2000:2100,]\n\ntext_sample |&gt;\n  gt() |&gt;\n  tab_header(title = \"Sample of Text\") |&gt;\n  opt_stylize(style = 6, color = \"gray\") |&gt;\n  tab_options(container.height = \"300px\", container.overflow.y = \"scroll\")\n\n\n\n\n\n\n\nSample of Text\n\n\nbook\ntext\nline_num\n\n\n\n\nA Study In Scarlet\n\"'You had best tell me all about it now,' I said. 'Half-confidences\n2000\n\n\nA Study In Scarlet\nare worse than none. Besides, you do not know how much we know of\n2001\n\n\nA Study In Scarlet\nit.'\n2002\n\n\nA Study In Scarlet\nNA\n2003\n\n\nA Study In Scarlet\n\"'On your head be it, Alice!' cried her mother; and then, turning to\n2004\n\n\nA Study In Scarlet\nme, 'I will tell you all, sir. Do not imagine that my agitation on\n2005\n\n\nA Study In Scarlet\nbehalf of my son arises from any fear lest he should have had a hand\n2006\n\n\nA Study In Scarlet\nin this terrible affair. He is utterly innocent of it. My dread is,\n2007\n\n\nA Study In Scarlet\nhowever, that in your eyes and in the eyes of others he may appear to\n2008\n\n\nA Study In Scarlet\nbe compromised. That however is surely impossible. His high\n2009\n\n\nA Study In Scarlet\ncharacter, his profession, his antecedents would all forbid it.'\n2010\n\n\nA Study In Scarlet\nNA\n2011\n\n\nA Study In Scarlet\n\"'Your best way is to make a clean breast of the facts,' I answered.\n2012\n\n\nA Study In Scarlet\n'Depend upon it, if your son is innocent he will be none the worse.'\n2013\n\n\nA Study In Scarlet\nNA\n2014\n\n\nA Study In Scarlet\n\"'Perhaps, Alice, you had better leave us together,' she said, and\n2015\n\n\nA Study In Scarlet\nher daughter withdrew. 'Now, sir,' she continued, 'I had no intention\n2016\n\n\nA Study In Scarlet\nof telling you all this, but since my poor daughter has disclosed it\n2017\n\n\nA Study In Scarlet\nI have no alternative. Having once decided to speak, I will tell you\n2018\n\n\nA Study In Scarlet\nall without omitting any particular.'\n2019\n\n\nA Study In Scarlet\nNA\n2020\n\n\nA Study In Scarlet\n\"'It is your wisest course,' said I.\n2021\n\n\nA Study In Scarlet\nNA\n2022\n\n\nA Study In Scarlet\n\"'Mr. Drebber has been with us nearly three weeks. He and his\n2023\n\n\nA Study In Scarlet\nsecretary, Mr. Stangerson, had been travelling on the Continent. I\n2024\n\n\nA Study In Scarlet\nnoticed a \"Copenhagen\" label upon each of their trunks, showing that\n2025\n\n\nA Study In Scarlet\nthat had been their last stopping place. Stangerson was a quiet\n2026\n\n\nA Study In Scarlet\nreserved man, but his employer, I am sorry to say, was far otherwise.\n2027\n\n\nA Study In Scarlet\nHe was coarse in his habits and brutish in his ways. The very night\n2028\n\n\nA Study In Scarlet\nof his arrival he became very much the worse for drink, and, indeed,\n2029\n\n\nA Study In Scarlet\nafter twelve o'clock in the day he could hardly ever be said to be\n2030\n\n\nA Study In Scarlet\nsober. His manners towards the maid-servants were disgustingly free\n2031\n\n\nA Study In Scarlet\nand familiar. Worst of all, he speedily assumed the same attitude\n2032\n\n\nA Study In Scarlet\ntowards my daughter, Alice, and spoke to her more than once in a way\n2033\n\n\nA Study In Scarlet\nwhich, fortunately, she is too innocent to understand. On one\n2034\n\n\nA Study In Scarlet\noccasion he actually seized her in his arms and embraced her--an\n2035\n\n\nA Study In Scarlet\noutrage which caused his own secretary to reproach him for his\n2036\n\n\nA Study In Scarlet\nunmanly conduct.'\n2037\n\n\nA Study In Scarlet\nNA\n2038\n\n\nA Study In Scarlet\n\"'But why did you stand all this,' I asked. 'I suppose that you can\n2039\n\n\nA Study In Scarlet\nget rid of your boarders when you wish.'\n2040\n\n\nA Study In Scarlet\nNA\n2041\n\n\nA Study In Scarlet\n\"Mrs. Charpentier blushed at my pertinent question. 'Would to God\n2042\n\n\nA Study In Scarlet\nthat I had given him notice on the very day that he came,' she said.\n2043\n\n\nA Study In Scarlet\n'But it was a sore temptation. They were paying a pound a day\n2044\n\n\nA Study In Scarlet\neach--fourteen pounds a week, and this is the slack season. I am a\n2045\n\n\nA Study In Scarlet\nwidow, and my boy in the Navy has cost me much. I grudged to lose the\n2046\n\n\nA Study In Scarlet\nmoney. I acted for the best. This last was too much, however, and I\n2047\n\n\nA Study In Scarlet\ngave him notice to leave on account of it. That was the reason of his\n2048\n\n\nA Study In Scarlet\ngoing.'\n2049\n\n\nA Study In Scarlet\nNA\n2050\n\n\nA Study In Scarlet\n\"'Well?'\n2051\n\n\nA Study In Scarlet\nNA\n2052\n\n\nA Study In Scarlet\n\"'My heart grew light when I saw him drive away. My son is on leave\n2053\n\n\nA Study In Scarlet\njust now, but I did not tell him anything of all this, for his temper\n2054\n\n\nA Study In Scarlet\nis violent, and he is passionately fond of his sister. When I closed\n2055\n\n\nA Study In Scarlet\nthe door behind them a load seemed to be lifted from my mind. Alas,\n2056\n\n\nA Study In Scarlet\nin less than an hour there was a ring at the bell, and I learned that\n2057\n\n\nA Study In Scarlet\nMr. Drebber had returned. He was much excited, and evidently the\n2058\n\n\nA Study In Scarlet\nworse for drink. He forced his way into the room, where I was sitting\n2059\n\n\nA Study In Scarlet\nwith my daughter, and made some incoherent remark about having missed\n2060\n\n\nA Study In Scarlet\nhis train. He then turned to Alice, and before my very face, proposed\n2061\n\n\nA Study In Scarlet\nto her that she should fly with him. \"You are of age,\" he said, \"and\n2062\n\n\nA Study In Scarlet\nthere is no law to stop you. I have money enough and to spare. Never\n2063\n\n\nA Study In Scarlet\nmind the old girl here, but come along with me now straight away. You\n2064\n\n\nA Study In Scarlet\nshall live like a princess.\" Poor Alice was so frightened that she\n2065\n\n\nA Study In Scarlet\nshrunk away from him, but he caught her by the wrist and endeavoured\n2066\n\n\nA Study In Scarlet\nto draw her towards the door. I screamed, and at that moment my son\n2067\n\n\nA Study In Scarlet\nArthur came into the room. What happened then I do not know. I heard\n2068\n\n\nA Study In Scarlet\noaths and the confused sounds of a scuffle. I was too terrified to\n2069\n\n\nA Study In Scarlet\nraise my head. When I did look up I saw Arthur standing in the\n2070\n\n\nA Study In Scarlet\ndoorway laughing, with a stick in his hand. \"I don't think that fine\n2071\n\n\nA Study In Scarlet\nfellow will trouble us again,\" he said. \"I will just go after him and\n2072\n\n\nA Study In Scarlet\nsee what he does with himself.\" With those words he took his hat and\n2073\n\n\nA Study In Scarlet\nstarted off down the street. The next morning we heard of Mr.\n2074\n\n\nA Study In Scarlet\nDrebber's mysterious death.'\n2075\n\n\nA Study In Scarlet\nNA\n2076\n\n\nA Study In Scarlet\n\"This statement came from Mrs. Charpentier's lips with many gasps and\n2077\n\n\nA Study In Scarlet\npauses. At times she spoke so low that I could hardly catch the\n2078\n\n\nA Study In Scarlet\nwords. I made shorthand notes of all that she said, however, so that\n2079\n\n\nA Study In Scarlet\nthere should be no possibility of a mistake.\"\n2080\n\n\nA Study In Scarlet\nNA\n2081\n\n\nA Study In Scarlet\n\"It's quite exciting,\" said Sherlock Holmes, with a yawn. \"What\n2082\n\n\nA Study In Scarlet\nhappened next?\"\n2083\n\n\nA Study In Scarlet\nNA\n2084\n\n\nA Study In Scarlet\n\"When Mrs. Charpentier paused,\" the detective continued, \"I saw that\n2085\n\n\nA Study In Scarlet\nthe whole case hung upon one point. Fixing her with my eye in a way\n2086\n\n\nA Study In Scarlet\nwhich I always found effective with women, I asked her at what hour\n2087\n\n\nA Study In Scarlet\nher son returned.\n2088\n\n\nA Study In Scarlet\nNA\n2089\n\n\nA Study In Scarlet\n\"'I do not know,' she answered.\n2090\n\n\nA Study In Scarlet\nNA\n2091\n\n\nA Study In Scarlet\n\"'Not know?'\n2092\n\n\nA Study In Scarlet\nNA\n2093\n\n\nA Study In Scarlet\n\"'No; he has a latch-key, and he let himself in.'\n2094\n\n\nA Study In Scarlet\nNA\n2095\n\n\nA Study In Scarlet\n\"'After you went to bed?'\n2096\n\n\nA Study In Scarlet\nNA\n2097\n\n\nA Study In Scarlet\n\"'Yes.'\n2098\n\n\nA Study In Scarlet\nNA\n2099\n\n\nA Study In Scarlet\n\"'When did you go to bed?'\n2100\n\n\n\n\n\n\n\n  The most remarkable thing from the sample is that each observation represents a literal line in the novel. The lines can be concatenated so that each observation is a paragraph. This will be particularly useful in understanding dialog because it is typical in literature to start a new paragraph each time a speaker changes. The NAs between lines mean we have a good boundary between paragraphs.\nAdditionally, I cross-referenced the novel order with the Canon of Sherlock Holmes in order to understand the order in which these works were published. I think it could be interesting to see if there are changes that occur between earlier works and later works.  \n\nparagraphs &lt;- data |&gt;\n  mutate(paragraph = cumsum(is.na(text))) |&gt;\n  filter(!is.na(text)) |&gt;\n  group_by(book,paragraph) |&gt;\n  summarize(text = paste(text, collapse = \" \"))\n\n`summarise()` has grouped output by 'book'. You can override using the\n`.groups` argument.\n\nparagraphs_sample &lt;- paragraphs[2000:2010,]\n\nparagraphs_sample |&gt;\n  gt() |&gt;\n  tab_header(title = \"Sample of Paragraphs\") |&gt;\n  opt_stylize(style = 6, color = \"gray\") |&gt;\n  tab_options(container.height = \"300px\", container.overflow.y = \"scroll\")\n\n\n\n\n\n\n\nSample of Paragraphs\n\n\nparagraph\ntext\n\n\n\n\nThe Adventure of Charles Augustus Milverton\n\n\n7455\n\"What I say is true,\" Holmes answered. \"The money cannot be found. Surely it is better for you to take the substantial sum which I offer than to ruin this woman's career, which can profit you in no way?\"\n\n\n7456\n\"There you make a mistake, Mr. Holmes. An exposure would profit me indirectly to a considerable extent. I have eight or ten similar cases maturing. If it was circulated among them that I had made a severe example of the Lady Eva I should find all of them much more open to reason. You see my point?\"\n\n\n7457\nHolmes sprang from his chair.\n\n\n7458\n\"Get behind him, Watson! Don't let him out! Now, sir, let us see the contents of that note-book.\"\n\n\n7459\nMilverton had glided as quick as a rat to the side of the room, and stood with his back against the wall.\n\n\n7460\n\"Mr. Holmes, Mr. Holmes,\" he said, turning the front of his coat and exhibiting the butt of a large revolver, which projected from the inside pocket. \"I have been expecting you to do something original. This has been done so often, and what good has ever come from it? I assure you that I am armed to the teeth, and I am perfectly prepared to use my weapons, knowing that the law will support me. Besides, your supposition that I would bring the letters here in a note-book is entirely mistaken. I would do nothing so foolish. And now, gentlemen, I have one or two little interviews this evening, and it is a long drive to Hampstead.\" He stepped forward, took up his coat, laid his hand on his revolver, and turned to the door. I picked up a chair, but Holmes shook his head and I laid it down again. With bow, a smile, and a twinkle Milverton was out of the room, and a few moments after we heard the slam of the carriage door and the rattle of the wheels as he drove away.\n\n\n7461\nHolmes sat motionless by the fire, his hands buried deep in his trouser pockets, his chin sunk upon his breast, his eyes fixed upon the glowing embers. For half an hour he was silent and still. Then, with the gesture of a man who has taken his decision, he sprang to his feet and passed into his bedroom. A little later a rakish young workman with a goatee beard and a swagger lit his clay pipe at the lamp before descending into the street. \"I'll be back some time, Watson,\" said he, and vanished into the night. I understood that he had opened his campaign against Charles Augustus Milverton; but I little dreamed the strange shape which that campaign was destined to take.\n\n\n7462\nFor some days Holmes came and went at all hours in this attire, but beyond a remark that his time was spent at Hampstead, and that it was not wasted, I knew nothing of what he was doing. At last, however, on a wild, tempestuous evening, when the wind screamed and rattled against the windows, he returned from his last expedition, and having removed his disguise he sat before the fire and laughed heartily in his silent inward fashion.\n\n\n7463\n\"You would not call me a marrying man, Watson?\"\n\n\n7464\n\"No, indeed!\"\n\n\n7465\n\"You'll be interested to hear that I am engaged.\"\n\n\n\n\n\n\n\n \n\n\n\nThe analysis from Blatt that I‚Äôm interested in replicating is his exclamation point analysis.\nF. Scott Fitzgerald and Earnest Hemingway were known for their very resvered use of exclamation points. Fizgerald describes it as ‚ÄúLaughing at your own joke.‚Äù We can use this data to determine how well Arthur Conan Doyle follows this rule.  \n\nexclamation_points &lt;- paragraphs |&gt;\n  mutate(exclamation_points = str_count(text, \"!\"))\n\nexclamation_point_sample &lt;- exclamation_points |&gt;\n  filter(exclamation_points &gt; 0) |&gt;\n  ungroup() |&gt;\n  slice_sample(n = 5)\n\n### showing some example lines as a sanity check to make sure our code is working\n\nexclamation_point_sample |&gt;\n  gt() |&gt;\n  tab_header(title = \"Sample of Lines with Exclamation Points\") |&gt;\n  opt_stylize(style = 6, color = \"gray\") |&gt;\n  tab_options(container.height = \"300px\", container.overflow.y = \"scroll\")\n\n\n\n\n\n\n\nSample of Lines with Exclamation Points\n\n\nbook\nparagraph\ntext\nexclamation_points\n\n\n\n\nThe Adventure of the Noble Bachelor\n3585\n\"Without, however, the knowledge of pre-existing cases which serves me so well. There was a parallel instance in Aberdeen some years back, and something on very much the same lines at Munich the year after the Franco-Prussian War. It is one of these cases--but, hullo, here is Lestrade! Good-afternoon, Lestrade! You will find an extra tumbler upon the sideboard, and there are cigars in the box.\"\n2\n\n\nThe Adventure of the Devil's Foot\n13067\n\"It won't do, Watson!\" said he with a laugh. \"Let us walk along the cliffs together and search for flint arrows. We are more likely to find them than clues to this problem. To let the brain work without sufficient material is like racing an engine. It racks itself to pieces. The sea air, sunshine, and patience, Watson--all else will come.\n1\n\n\nThe Adventure of the Six Napoleons\n7640\n\"Splendid!\"\n1\n\n\nThe Man with the Twisted Lip\n2582\n\"Why,\" said my wife, pulling up her veil, \"it is Kate Whitney. How you startled me, Kate! I had not an idea who you were when you came in.\"\n1\n\n\nThe Sign of the Four\n1026\n\"It means murder,\" said he, stooping over the dead man. \"Ah, I expected it. Look here!\" He pointed to what looked like a long, dark thorn stuck in the skin just above the ear.\n1\n\n\n\n\n\n\nexclamation_points_summary &lt;- exclamation_points |&gt;\n  group_by(book) |&gt;\n  summarize(exclamation_points = sum(exclamation_points)) |&gt;\n  left_join(books, by = \"book\") |&gt;\n  arrange(publish_order)\n\n### actually counting exclamation points\n\nexclamation_points_summary |&gt;\n  gt() |&gt;\n  tab_header(title = \"Exclamation Points per Book\") |&gt;\n  tab_options(container.height = \"500px\", container.overflow.y = \"scroll\") |&gt;\n  cols_label(\n    book = \"Book\",\n    exclamation_points = \"Exclamation Points\",\n    publish_order = \"Publish Order\"\n  ) |&gt;\n  fmt_number(columns = exclamation_points, decimals = 0)\n\n\n\n\n\n\n\nExclamation Points per Book\n\n\nBook\nExclamation Points\nPublish Order\n\n\n\n\nA Study In Scarlet\n85\n1\n\n\nThe Sign of the Four\n127\n2\n\n\nA Scandal in Bohemia\n33\n3\n\n\nThe Red-Headed League\n16\n4\n\n\nA Case of Identity\n13\n5\n\n\nThe Boscombe Valley Mystery\n27\n6\n\n\nThe Five Orange Pips\n17\n7\n\n\nThe Man with the Twisted Lip\n32\n8\n\n\nThe Adventure of the Blue Carbuncle\n40\n9\n\n\nThe Adventure of the Speckled Band\n34\n10\n\n\nThe Adventure of the Engineer's Thumb\n31\n11\n\n\nThe Adventure of the Noble Bachelor\n19\n12\n\n\nThe Adventure of the Beryl Coronet\n43\n13\n\n\nThe Adventure of the Copper Beeches\n40\n14\n\n\nSilver Blaze\n36\n15\n\n\nThe Yellow Face\n12\n16\n\n\nThe Stock-Broker's Clerk\n29\n17\n\n\nThe \"Gloria Scott\"\n19\n18\n\n\nThe Musgrave Ritual\n7\n19\n\n\nThe Reigate Squires\n24\n20\n\n\nThe Crooked Man\n15\n21\n\n\nThe Resident Patient\n17\n22\n\n\nThe Greek Interpreter\n13\n23\n\n\nThe Naval Treaty\n34\n24\n\n\nThe Final Problem\n10\n25\n\n\nThe Adventure of the Empty House\n16\n26\n\n\nThe Adventure of the Norwood Builder\n24\n27\n\n\nThe Adventure of the Dancing Men\n17\n28\n\n\nThe Adventure of the Solitary Cyclist\n48\n29\n\n\nThe Adventure of the Priory School\n36\n30\n\n\nThe Adventure of Black Peter\n10\n31\n\n\nThe Adventure of Charles Augustus Milverton\n24\n32\n\n\nThe Adventure of the Six Napoleons\n17\n33\n\n\nThe Adventure of the Three Students\n23\n34\n\n\nThe Adventure of the Golden Pince-Nez\n36\n35\n\n\nThe Adventure of the Missing Three-Quarter\n34\n36\n\n\nThe Adventure of the Abbey Grange\n21\n37\n\n\nThe Adventure of the Second Stain\n49\n38\n\n\nThe Hound of the Baskervilles\n182\n39\n\n\nThe Valley Of Fear\n319\n40\n\n\nThe Adventure of Wisteria Lodge\n12\n41\n\n\nThe Adventure of the Cardboard Box\n20\n42\n\n\nThe Adventure of the Red Circle\n46\n43\n\n\nThe Adventure of the Bruce-Partington Plans\n43\n44\n\n\nThe Adventure of the Dying Detective\n45\n45\n\n\nThe Disappearance of Lady Frances Carfax\n40\n46\n\n\nThe Adventure of the Devil's Foot\n18\n47\n\n\nHis Last Bow\n23\n48\n\n\n\n\n\n\n\n  Noteably the 4 highest counts are full novels, while the rest are short stories. It makes sense that they are higher, but they will skew the rest of the analysis.\nIn graphical form:  \n\nnovels &lt;- c(\"A Study In Scarlet\",\n            \"The Sign of the Four\",\n            \"The Hound of the Baskervilles\",\n            \"The Valley Of Fear\" )\n\nexclamation_points_summary |&gt;\n  filter(!(book %in% novels)) |&gt;\n  ggplot(aes(x = publish_order, y = exclamation_points)) +\n  geom_point() +\n  labs(\n    x = \"Publish Order\",\n    y = \"Exclamation Points\",\n    title = \"Exclamation Points per Book\"\n  )\n\n\n\n\n\n\n\n\n  Graphically, it does not look like Doyle‚Äôs use of exclamation points changed over time. We can verify this mathematically.  \n\nexclamation_points_summary |&gt;\n  filter(!(book %in% novels)) |&gt;\n  lm(exclamation_points ~ publish_order, data = _) |&gt;\n  summary()\n\n\nCall:\nlm(formula = exclamation_points ~ publish_order, data = filter(exclamation_points_summary, \n    !(book %in% novels)))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-18.637 -10.027  -3.060   9.545  21.007 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    23.0608     3.8057   6.060 3.26e-07 ***\npublish_order   0.1356     0.1351   1.003    0.321    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.86 on 42 degrees of freedom\nMultiple R-squared:  0.02341,   Adjusted R-squared:  0.0001549 \nF-statistic: 1.007 on 1 and 42 DF,  p-value: 0.3214\n\n\n \nSometimes there‚Äôs not anything particularly exciting with a type of analysis; sometimes just seeing that he remains consistent throughout is career as an awesome insight in itself.\n\n\n\nI received an excellent suggestion around normalization: \n\n\nExcellent!! It would be cool to see exclamation marks divided by word count! Since longer works will have more by nature! How many exclamation marks can I get in this post?! üòÇ\n\n‚Äî Libby Heeren (@libbyheeren.bsky.social) November 24, 2025 at 10:02 AM\n\n\n\nIndeed, just because I only included the short stories in the previous visualisation does not mean that all of Doyle‚Äôs short stories are same length, and therefore, the absolute count comparision might not necessarily be fair. That said, there is one more method for normalization that is also worth considering. Hemingway would argue that most exclamation points should be replaced with periods. In addition to the above suggestion, I will also normalize by the dividing the sum of terminal periods and exclamation points. This one is a bit more tricky. I don‚Äôt want to count ‚ÄúDr.‚Äù or ‚Äú1.‚Äù as sentences. To correct for this, I will count periods immediately after a word that has at least one lowercase vowel.\n\nexclamation_points_normalization &lt;- paragraphs |&gt;\n  mutate(exclamation_points = str_count(text, \"!\"),\n         words = lengths(str_split(text, \" \")),\n         terminal_periods = str_count(text, \"\\\\b\\\\w*[aeiou]\\\\w*\\\\.\") #thanks chatGPT\n         )\nsample &lt;- exclamation_points_normalization |&gt;\n  ungroup() |&gt;\n  filter(exclamation_points &gt; 0) |&gt;\n  filter(terminal_periods &gt; 0) |&gt;\n  slice_sample(n=5)\n\nsample |&gt; #Trust but verify\n  gt() |&gt;\n  tab_header(title = \"Sample of Punctuation and Word Counts\") |&gt;\n  opt_stylize(style = 6, color = \"gray\") |&gt;\n  tab_options(container.height = \"300px\", container.overflow.y = \"scroll\") \n\n\n\n\n\n\n\nSample of Punctuation and Word Counts\n\n\nbook\nparagraph\ntext\nexclamation_points\nwords\nterminal_periods\n\n\n\n\nThe Adventure of the Solitary Cyclist\n6820\n\"Enough of this,\" said my friend, coldly. \"Drop that pistol! Watson, pick it up! Hold it to his head! Thank you. You, Carruthers, give me that revolver. We'll have no more violence. Come, hand it over!\"\n4\n36\n4\n\n\nThe Adventure of the Golden Pince-Nez\n8009\n\"Tobacco and my work, but now only tobacco,\" the old man exclaimed. \"Alas! what a fatal interruption! Who could have foreseen such a terrible catastrophe? So estimable a young man! I assure you that after a few months' training he was an admirable assistant. What do you think of the matter, Mr. Holmes?\"\n3\n53\n2\n\n\nThe Hound of the Baskervilles\n8905\n\"Excellent! This is a colleague, Watson, after our own heart. But the marks?\"\n1\n13\n1\n\n\nA Study In Scarlet\n50\n\"Indeed!\" I murmured.\n1\n3\n1\n\n\nThe Hound of the Baskervilles\n10056\n\"To see Sir Henry. Ah, here he is!\"\n1\n8\n1\n\n\n\n\n\n\n\n \n\nexclamation_points_normalization_summary &lt;- exclamation_points_normalization |&gt;\n  group_by(book) |&gt;\n  summarize(exclamation_points = sum(exclamation_points),\n            terminal_periods = sum(terminal_periods),\n            words = sum(words),\n            exclamation_point_per_word = exclamation_points / words,\n            exclamation_point_per_sentence_ending = exclamation_points / (terminal_periods + exclamation_points)) |&gt;\n  left_join(books, by = \"book\")\n\nmetrics &lt;- tibble(\n  Metric = c(\"Most Exclamations\", \"Least Exclamations\", \"Most Exclamations per Word\", \"Least Exclamations per Word\", \"Most Exclamations per Sentence Ending\", \"Least Exclamations per Sentence Ending\"),\n  book = c(\n    exclamation_points_normalization_summary$book[which.max(exclamation_points_normalization_summary$exclamation_points)],\n    exclamation_points_normalization_summary$book[which.min(exclamation_points_normalization_summary$exclamation_points)],\n    exclamation_points_normalization_summary$book[which.max(exclamation_points_normalization_summary$exclamation_point_per_word)],\n    exclamation_points_normalization_summary$book[which.min(exclamation_points_normalization_summary$exclamation_point_per_word)],\n    exclamation_points_normalization_summary$book[which.max(exclamation_points_normalization_summary$exclamation_point_per_sentence_ending)],\n    exclamation_points_normalization_summary$book[which.min(exclamation_points_normalization_summary$exclamation_point_per_sentence_ending)]\n  ),\n  Value = c(\n    max(exclamation_points_normalization_summary$exclamation_points),\n    min(exclamation_points_normalization_summary$exclamation_points),\n    max(exclamation_points_normalization_summary$exclamation_point_per_word),\n    min(exclamation_points_normalization_summary$exclamation_point_per_word),\n    max(exclamation_points_normalization_summary$exclamation_point_per_sentence_ending),\n    min(exclamation_points_normalization_summary$exclamation_point_per_sentence_ending)\n  )\n)\n\nmetrics |&gt;\n  gt() |&gt;\n  cols_label(\n    book = \"Book\",\n    Value = \"Value\"\n  ) |&gt;\n  fmt_number(columns = Value, decimals = 4) |&gt;\n  tab_header(title = \"Exclamation Point Usage Summary\")\n\n\n\n\n\n\n\nExclamation Point Usage Summary\n\n\nMetric\nBook\nValue\n\n\n\n\nMost Exclamations\nThe Valley Of Fear\n319.0000\n\n\nLeast Exclamations\nThe Musgrave Ritual\n7.0000\n\n\nMost Exclamations per Word\nThe Adventure of the Dying Detective\n0.0078\n\n\nLeast Exclamations per Word\nThe Musgrave Ritual\n0.0009\n\n\nMost Exclamations per Sentence Ending\nThe Adventure of the Solitary Cyclist\n0.1004\n\n\nLeast Exclamations per Sentence Ending\nThe Adventure of Wisteria Lodge\n0.0172\n\n\n\n\n\n\n\n \n\nexclamation_points_normalization_summary_for_plot &lt;- exclamation_points_normalization_summary |&gt;\n  pivot_longer(cols = c(exclamation_point_per_word, exclamation_point_per_sentence_ending)) |&gt;\n  mutate(name = recode(name,\n                         \"exclamation_point_per_word\" = \"Per Word\",\n                         \"exclamation_point_per_sentence_ending\" = \"Per Sentence Ending\"))\n\nexclamation_normalization_plot &lt;- exclamation_points_normalization_summary_for_plot |&gt;\n  ggplot(aes(x = publish_order, y = value)) +\n  facet_wrap(~name, scales = \"free_y\") +\n  geom_point() +\n  labs(x = \"Publish Order\",\n       y = NULL,\n       title = \"Normalized Exclamation Point Usage\")\nexclamation_normalization_plot"
  },
  {
    "objectID": "posts/2025-12-20-ekg-digitization/index.html",
    "href": "posts/2025-12-20-ekg-digitization/index.html",
    "title": "EKG Digitization",
    "section": "",
    "text": "I occasionally peruse available Kaggle competitions to see if there‚Äôs something fun to explore. This week, I came across this competition around converting EKG images into time series data. This piqued my interest because context can give such a big advantage over a pure ML approach. I‚Äôm going to walk through some basics of EKGs and hopefully we can extract some time series data without using any ML. That said, there is obviously a place for computer vision to do a lot of the work that I‚Äôm going to write more deterministic code for.\n\nReyna et al. (2025)\n\nRed squares\nBefore even looking at the actual signal, I‚Äôm interested in the red lines. Here‚Äôs a good explanation from Wikipedia. We can use the red line to build our coordinate system and from there, identify time and voltage values for the actual ekg signal.\n\nKuhn, Stannered, and MoodyGroove (2007)\nFirst, let‚Äôs pull out the red channel.\n\n\n\n\n\n\nFigure¬†1: Red channel of ECG grid\n\n\n\nI was surprised to see that we pretty much eliminated the grid here. This is still helpful though. Lets find a line with no EKG signal and no writing.\n\n## convert to a matrix\n\nred_matrix &lt;- as.integer(image_data(red))\n\n## find the number of pixels in each row with &gt;200 value\n\nhigh_value_rows &lt;- red_matrix[,,1] |&gt;\n  apply(1, function(x) sum(x &gt; 200))\nhigh_value_rows &lt;- tibble(count = high_value_rows) |&gt;\n  mutate(row = max(row_number()) - row_number() + 1)\n\nempty_row_graph &lt;- ggplot(high_value_rows, aes(x = row, y = count)) +\n  coord_flip() +\n  annotation_raster(red, xmin = 0, xmax = 1700, ymin = 0, ymax = 2200) +\n  geom_line(color = \"red\") +\n  geom_vline(aes(xintercept = 1298), color = \"blue\")\n    \nempty_row_graph\n\n\n\n\n\n\n\n\nFrom the plot overlay, we can now see where there is writing and signal and where there is nothing but grid. Let‚Äôs now find a horizontal line without signal. I added a blue line that I‚Äôll use to measure my grid. I‚Äôll use the blue channel for this one.\n\n\n\n\n\n\nFigure¬†2: Blue channel of ECG grid\n\n\n\n\nblue_matrix &lt;- as.integer(image_data(blue))\n\n\nhorizontal_lines &lt;- blue_matrix[502, , 1]\n## create a 3 column tibble with one column being the row number, the second column number, and the third column the value\nhorizontal_lines &lt;- tibble(\n  column = seq(1, 2200),\n  row = 502,\n  value = horizontal_lines)\ngrid_plot &lt;- ggplot(horizontal_lines, aes(x = column, y = value)) +\n  geom_line()\n\ngrid_plot\n\n\n\n\n\n\n\n\nlet‚Äôs zoom in and add the blue channel underneath.\n\ncrop_blue &lt;- image_crop(blue, \"300x95+1200+450\")\n\nzoomed_in &lt;- grid_plot +\n  lims(x = c(1200, 1500), y = c(0, 350)) +\n  annotation_raster(crop_blue, xmin = 1200, xmax = 1500, ymin = 255, ymax = 350)\n\nzoomed_in\n\n\n\n\n\n\n\n\nEverything lines up perfectly. We can take the indeces of our minima and calculate a time per pixel for our x-axis and a mV per pixel for our y-axis.\n\nhorizontal_lines_minima &lt;- horizontal_lines |&gt;\n  filter(!(column %in% c(1, 2200))) |&gt;\n  filter(value &lt; 20) |&gt;\n  mutate(next_minima = lead(column)) |&gt;\n  filter(!is.na(next_minima)) |&gt;\n  mutate(diff = next_minima - column)\n\nbig_box_width &lt;- mean(horizontal_lines_minima$diff)\n\nThe width of one big box is 39.4 pixels, which is equal to 200ms horizontally and 0.5mV vertically. Now let‚Äôs start tracing the signal itself.\n\n\nFinding the signals\nFor this, we‚Äôre going to go back to the red channel. We‚Äôll use our previous analysis that we used to find rows that have no signal, and instead use it to find the rows that have our signals. There are 4 rows of signals. The first three have all 12 leads, and the bottom row is lead II going all the way across to be used for rate and rhythm.\n\nbands_of_signal &lt;- high_value_rows |&gt;\n  filter(count &lt; max(count)) |&gt;\n  mutate(next_row = lead(row)) |&gt;\n  filter(!is.na(next_row)) |&gt;\n  mutate(band = row - next_row == 1) |&gt;\n  group_by(temp = cumsum(!lag(band, default = TRUE))) |&gt;\n  mutate(band = cur_group_id()) |&gt;\n  ungroup() |&gt;\n  select(-temp) |&gt;\n  group_by(band) |&gt;\n  summarize(\n    start = min(row),\n    end = max(row),\n    zero_volatage = row[which.min(count)])\n\n\nbands_of_signal_graph &lt;- ggplot(bands_of_signal |&gt; filter(band &gt;= 5 & band &lt;= 8) |&gt; mutate(band = as.factor(band)), aes(labels = band, xmin = start, xmax = end, ymin = 0, ymax = 2200)) +\n    annotation_raster(red, xmin = 0, xmax = 1700, ymin = 0, ymax = 2200) +\n    geom_rect(aes(fill = band), alpha = 0.5) +\n    geom_vline(aes(xintercept = zero_volatage), color = \"red\") +\n    coord_flip() +\n    ylim(c(0, 2200)) +\n    xlim(c(0, 1700))\n\n\n\nbands_of_signal_graph\n\n\n\n\n\n\n\n\nNow we‚Äôve found the parts of the image that have signal and we also identified the zero voltage line. Technically, zero voltage is defined as the voltage between the T wave and the following P wave, but this approximation of counting dark pixels in each row and identifying a local maximum seems to work incredibly well. You‚Äôll notice that the bands are labeled 5-8 because I started at the top and each section of text or signal created a band. This will not always be consistent. In lead II, you can see that the label is a new band, but in the other rows, the label is included in the band. Depending on the patient, the device, and the strength of the lead connection, the amplitude of the signal can vary and overlap with the label (sometimes even overlap with the signal above or below).\n\n\nTracing the signal\nLet‚Äôs see if we can trace the signal on the red channel. The naive approach is to find the location of the min value in each column, and when there is more than one, take the highest location. I‚Äôll also block out where the letters are so we can avoid those.\n\nleads &lt;- lapply(seq(5,8), function(x) {\nband &lt;- red_matrix[(1700 - bands_of_signal$end[x]):(1700 - bands_of_signal$start[x]),,1]\nif(x != 8) {\n  band[length(band[,1]):(length(band[,1])-42), 100:150] &lt;- 255\n  band[length(band[,1]):(length(band[,1])-42), 600:675] &lt;- 255\n  band[length(band[,1]):(length(band[,1])-42), 1090:1150] &lt;- 255\n  band[length(band[,1]):(length(band[,1])-42), 1590:1650] &lt;- 255\n}\nband_tibble &lt;- tibble(\n  column = seq(1, 2200)) |&gt;\nmutate(row = map_int(column, function(y) bands_of_signal$end[x] - min(which(band[,y] == min(band[,y]))))) |&gt;\nmutate(band = x)\n    }) |&gt;\n    bind_rows()\n\n\nbands_graph &lt;- ggplot(leads, aes(x = column, y = row, color = as.factor(band))) +\n  annotation_raster(red, xmin = 0, xmax = 2200, ymin = 0, ymax = 1700) +\n  geom_line() +\n  xlim(c(0, 2200)) +\n  ylim(c(0, 1700)) +\n  scale_color_discrete(name = \"Band\")\n\n\nbands_graph\n\n\n\n\n\n\n\n\n\n\nConverting to time series\nNow that we have our signal traced, converting it to a time series is easy. We just need to subtract the zero voltage and convert the x and y coordinates into ms and mV, respectively.\n\nleads_time_series &lt;- leads |&gt;\n  left_join(bands_of_signal, by = \"band\") |&gt;\n  mutate(\n    mV = (row - zero_volatage)/big_box_width * 0.5,\n    ms = column/big_box_width * 200)\n\n\n  lead_time_series_plot &lt;- ggplot(leads_time_series, aes(x = ms, y = mV)) +\n    geom_line() +\n    facet_grid(rows = vars(band))\n\n  lead_time_series_plot\n\n\n\n\n\n\n\n\nNow our EKG is 10 seconds long. We could probably take it further by breaking it down into each individual lead and trimming the false signal along the edges, but I think we‚Äôve demonstrated how we can get this done with image processing and some basic knowledge about EKGs.\nThat said, there is definitely opportunity to improve this process with machine learning. We used a pretty standard PNG. The competition also includes paper scans, pictures taken with cell phones, black and white scans, etc. A robust process would be to use machine learning to get the EKG oriented correctly, colorize it into something consistent, and mark regions where the signal is and isn‚Äôt (e.g.¬†making sure the aVR label doesn‚Äôt get interpreted as signal).\n\n\n\n\n\nReferences\n\nKuhn, Markus, Stannered, and MoodyGroove. 2007. ECG Paper. https://commons.wikimedia.org/wiki/File:ECG_Paper.jpg.\n\n\nReyna, Matthew A., Deepanshi, James Weigle, Zuzana Koscova, Kiersten Campbell, Salman Seyedi, Andoni Elola, et al. 2025. ‚ÄúPhysioNet: Digitization of ECG Images.‚Äù Kaggle. 2025. https://kaggle.com/competitions/physionet-ecg-image-digitization."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "mark michael",
    "section": "",
    "text": "Deploying Innovation: Making Healthcare Technology Billable and Integrated\n\n\n\nhealth\n\nbilling\n\nproduct\n\n\n\n\n\n\n\n\n\nFeb 14, 2026\n\n\n\n\n\n\n\n\n\n\n\n\nTidyTuesday 2026-02-03 Edible Plants\n\n\n\ndata science\n\ntidytuesday\n\nrstats\n\n\n\n\n\n\n\n\n\nFeb 3, 2026\n\n\n\n\n\n\n\n\n\n\n\n\nEKG Digitization\n\n\n\nhealth\n\ndata processing\n\nrstats\n\n\n\n\n\n\n\n\n\nDec 20, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nThanksgiving, but make it Gantt\n\n\n\ncooking\n\nmermaid\n\nfun\n\n\n\n\n\n\n\n\n\nNov 29, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nTidyTuesday 2025-11-18 - Sherlock Holmes\n\n\n\ndata science\n\ntidytuesday\n\nrstats\n\n\n\n\n\n\n\n\n\nNov 22, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nFrom the Furrows - building a grownup backend\n\n\n\ndevelopment\n\nharvvest\n\n\n\n\n\n\n\n\n\nNov 15, 2025\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi, I‚Äôm Mark. I‚Äôm a Seattle-based product manager with a background in medicine. I have opinions about healthcare, technology, cycling, and other things.   Most recently, I‚Äôve been building Harvvest, a platform that helps influencers monetize their content by connecting them with brands for them to promote.  \nMD - Baylor College of Medicine  BE Biomedical Engineering - Vanderbilt University"
  },
  {
    "objectID": "posts/2025-11-29-thanksgiving_gantt/index.html",
    "href": "posts/2025-11-29-thanksgiving_gantt/index.html",
    "title": "Thanksgiving, but make it Gantt",
    "section": "",
    "text": "Thanksgiving is one of my favorite holidays. After reading the impassioned arguments of Brunch is Hell: How to save the world by throwing a dinner party, For about the past 10 years, I‚Äôve been hosting a friendsgiving and cooking mostly everything on my own (the past couple years I‚Äôve had some help and a good opportunity to practice delegation). It‚Äôs challenging, especially the last couple hours when several different things are in the oven and on the stove at the same time. The most important thing is to make a plan that you can refer to amidst the chaos.\n\n\n\n2025 Spread\n\n\nOne of my favorite organization tools is mermaid.js. It allows you to create various diagrams such as flow charts and gantt charts using code. This makes it really easy to edit because you can focus on the content and not have to worry about spacing, making sure arrows are straight, or selecting groups of objects to move down in order to make room for something in the middle. It also means that you can describe your process to ChatGPT or other AI tool of choice and it‚Äôll give you a pretty good start that you can just copy and paste into a markdown document.\nTo demonstrate both the power of mermaid.js and how you can prepare a potluck-less Friendsgiving, I‚Äôve diagrammed my process below and also included some reference recipes for the various dishes.\n\ngantt\n\n  title Thanksgiving Prep - Before Thanksgiving\n  dateFormat YYYY-MM-DD HH:mm\n  axisFormat %d-%H:%M\n  \n  section Turkey\n    Buy frozen turkey :milestone, 2025-11-18 14:30, 0d \n    Thaw turkey: t2, 2025-11-20 14:30, 4d\n    Dry brine: t4, 2025-11-24 14:30, 2d\n\n  section Cheesecake\n    Take cream cheese out of fridge :milestone, 2025-11-25 17:15, 0d \n    Crust: t15, 2025-11-26 18:00, 15m \n    Filling: t15a, 2025-11-26 18:15, 20m\n    Bake: t16, 2025-11-26 18:35, 75m\n\n\n\n\ngantt\n\n  title Thanksgiving Prep - Before Thanksgiving\n  dateFormat YYYY-MM-DD HH:mm\n  axisFormat %d-%H:%M\n  \n  section Turkey\n    Buy frozen turkey :milestone, 2025-11-18 14:30, 0d \n    Thaw turkey: t2, 2025-11-20 14:30, 4d\n    Dry brine: t4, 2025-11-24 14:30, 2d\n\n  section Cheesecake\n    Take cream cheese out of fridge :milestone, 2025-11-25 17:15, 0d \n    Crust: t15, 2025-11-26 18:00, 15m \n    Filling: t15a, 2025-11-26 18:15, 20m\n    Bake: t16, 2025-11-26 18:35, 75m \n\n\n\n\n\n\n\ngantt\n  title Thanksgiving - Day of\n  dateFormat HH:mm\n  axisFormat %H:%M\n  \n  section Turkey\n    Apply rub: t5, 2025-11-27 10:00, 30m\n    Smoke Turkey: t7, 2025-11-27 12:00, 4.5h\n    Rest Turkey: t8, 2025-11-27 16:30, 1.5h\n\n  section Mac and cheese\n    Prep sauce: 2025-11-27 16:15, 30m\n    Cook pasta: 2025-11-27 16:45, 10m\n    Assemble and add panko: 2025-11-27 16:55, 5m\n    smoke mac and cheese: 2025-11-27 17:00, 1h\n  \n  section Baked Brie\n    Prep Baked Brie: t13, 2025-11-27 17:00, 10m\n    Bake Baked Brie: t14, 2025-11-27 17:30, 25m\n\n  section Green Bean Casserole\n    Thaw green beans: t17c, 2025-11-27 08:00, 1h\n    Prepare Roux: t17a, 2025-11-27 10:00, 20m\n    Mix in green beans: t17b, 2025-11-27 10:20, 7m\n    Add French's Onions: 2025-11-27 17:10, 5m\n    Bake Casserole: t18, 2025-11-27 17:15, 45m\n\n  section Mashed Potatoes\n    Chop Potatoes: 2025-11-27 16:55, 15m\n    Boil Potatoes: 2025-11-27 17:10, 40m\n    Mash Potatoes: 2025-11-27 17:50, 10m\n\n  section Sweet Potatoes\n    Chop Potatoes: 2025-11-27 16:15, 15m\n    Boil Potatoes: 2025-11-27 16:30, 40m\n    Mash Potatoes: 2025-11-27 17:10, 10m\n    Prepare Casserole: 2025-11-27 17:20, 10m\n    Bake Casserole: 2025-11-27 17:30, 30m\n\n  section Stuffing\n    Chop and toast bread: 2025-11-27 10:30, 30m\n    Saute vegetables: 2025-11-27 14:00, 30m\n    Combine stock mixture, bread, and vegetables: 2025-11-27 17:00, 30m\n    Bake Stuffing: 2025-11-27 17:30, 30m\n\n  section Cranberry Sauce\n    Simmer cranberries, sugar, and spices: 2025-11-27 16:00, 30m\n    Cool: 2025-11-27 16:30, 1.5h\n\n\n\n\ngantt\n  title Thanksgiving - Day of\n  dateFormat HH:mm\n  axisFormat %H:%M\n  \n  section Turkey\n    Apply rub: t5, 2025-11-27 10:00, 30m\n    Smoke Turkey: t7, 2025-11-27 12:00, 4.5h\n    Rest Turkey: t8, 2025-11-27 16:30, 1.5h\n\n  section Mac and cheese\n    Prep sauce: 2025-11-27 16:15, 30m\n    Cook pasta: 2025-11-27 16:45, 10m\n    Assemble and add panko: 2025-11-27 16:55, 5m\n    smoke mac and cheese: 2025-11-27 17:00, 1h\n  \n  section Baked Brie\n    Prep Baked Brie: t13, 2025-11-27 17:00, 10m\n    Bake Baked Brie: t14, 2025-11-27 17:30, 25m\n\n  section Green Bean Casserole\n    Thaw green beans: t17c, 2025-11-27 08:00, 1h\n    Prepare Roux: t17a, 2025-11-27 10:00, 20m\n    Mix in green beans: t17b, 2025-11-27 10:20, 7m\n    Add French's Onions: 2025-11-27 17:10, 5m\n    Bake Casserole: t18, 2025-11-27 17:15, 45m\n\n  section Mashed Potatoes\n    Chop Potatoes: 2025-11-27 16:55, 15m\n    Boil Potatoes: 2025-11-27 17:10, 40m\n    Mash Potatoes: 2025-11-27 17:50, 10m\n\n  section Sweet Potatoes\n    Chop Potatoes: 2025-11-27 16:15, 15m\n    Boil Potatoes: 2025-11-27 16:30, 40m\n    Mash Potatoes: 2025-11-27 17:10, 10m\n    Prepare Casserole: 2025-11-27 17:20, 10m\n    Bake Casserole: 2025-11-27 17:30, 30m\n\n  section Stuffing\n    Chop and toast bread: 2025-11-27 10:30, 30m\n    Saute vegetables: 2025-11-27 14:00, 30m\n    Combine stock mixture, bread, and vegetables: 2025-11-27 17:00, 30m\n    Bake Stuffing: 2025-11-27 17:30, 30m\n\n  section Cranberry Sauce\n    Simmer cranberries, sugar, and spices: 2025-11-27 16:00, 30m\n    Cool: 2025-11-27 16:30, 1.5h\n\n\n\n\n\n\n\nImportant Notes\nIf you can find a refrigerated (not frozen), air-chilled turkey, go for that one. Antibiotic-free doesn‚Äôt mean anything, at least in the US, federal law requires all poultry to be antibiotic-free. Fancy heirloom turkeys are nice, but not $12/lb nice. If you hosting a Friendsgiving earlier in November, don‚Äôt wait until the last minute thinking you can find a refrigerated turkey unless you‚Äôve already identified and confirmed a source. They are definitely harder to find and the speed-thawing process for frozen turkey is painful.\nWatch this video before smoking your turkey. Aaron Franklin is truly a scholar in the world of smoked foods.\nI adapted the smoked mac and cheese recipe from here. I very rarely have leftovers.\nOther recipes:\n\nGreen Bean Casserole\nStuffing\n\nFor everything else, I mostly just winged it. Apply heat to the ingredients, and mix in some reasonable spices.\nImportantly, I smoked my turkey which I believe is the superior cooking method. A major stressor in Thanksgiving is that usually when the turkey is in the oven, nothing else can go in. Moving the turkey to an alternative cooking space (second oven, smoker, sous vide) makes things a lot easier.\nI hope this is helpful! Feel free to adapt this diagram for your plan and let me know how it goes!"
  },
  {
    "objectID": "posts/2025-11-15-from_the_furrows/index.html",
    "href": "posts/2025-11-15-from_the_furrows/index.html",
    "title": "From the Furrows - building a grownup backend",
    "section": "",
    "text": "Over a year ago, I committed my first lines of code for our new app. At that point I had used R for advanced data analysis work and dabbled into building my first applications. Armed with that and some product knowledge building health apps. Peter and I started on our Harvvest journey. We started assembling a list of brands with affiliate links and I created my first endpoint using plumber to show that list to the client.\nNow our app has become more complex. Users can preview brand deals to promote, generate their unique links, receive brand kits for the deals they want to promote, and track and withdraw their earnings. We brought on a more experienced backend developer and have started building the next version of our app, informed by the challenges from the previous year. One of the particularly interesting things about this type of growth is that some of the decisions that we are now considering suboptimal were actually pretty good for when we were getting started. And some of them were just bad. I‚Äôm going to go through some of those considerations here. \n\n\n\nPeter and I over a year into Harvvest with our first company swag\n\n\n\nWriting a backend in R\nThere is a lot of commentary for and against R in production environments. This article provides an excellent technical discussion on the topic. For an application like Harvvest, languages more tailored towards performance and concurrency rather than statistics would have been better. That said, it was the language I knew best. And because I had that knowledge, I could focus more on the programming methodology rather than banging my head against tons of syntax errors and blindly copying code for ChatGPT or StackOverflow. In fact, as we are transitioning to Python and FastAPI, we‚Äôre finding that probably around 85% of the endpoints and general functionality will be conserved. Also, I was able to build faster. When we planned for a new feature, I would typically have the backend and documentation completed within a day. Overall, I was happy with this choice in the beginning, and I think we still could have gotten more mileage from our R backend.\nI did do a couple of important changes along the way to account for some of R‚Äôs shortcomings. The first was that for some functionality, like sending a welcome email, it made sense to do it with a database function instead of via the backend. Accounts were created through direct interface with Supabase, so the database would know before the backend knew that an account had been created. Another change was that I moved functionality related to followers to a separate service written in Elixir. Our app would primarily be used by influencers. Their followers, who probably outnumber influencers by at least an order of magnitude, interact with the links that the influencers generate and post. With that change, our linking service was able to support our highest traffic days.\n\n\nDatabases\nMy background with databases until this point had been almost entirely querying data with SQL and working with it in R. I had never created tables or stored data in a database before. We made the decision to use Google Cloud because I had burned through my AWS free trials on a previous project. The natural option then became Firebase‚Äôs NoSQL database. Not only was I not familiar with interfacing with these types of databases, but I also could not find a mature R package to abstract the nuances away. I got to the point where I was using httr2 to populate brand deals into the Firebase before deciding that this process was not sustainable and I need to switch to something I was more familiar with‚ÄìPostgres.\nI ended up choosing Supabase without really understanding all the implications of that decision. Fortunately, it had benefits that I did not realize as I was making the decision. User creation and authentication are automatically taken care of, social logins were much easier to implement, and security was largely handled for me. Additionally, their free tier is plenty to prove out an idea.\nAn important lesson I learned is that using R to interface with a database for data analysis and data science vs building a consumer application. Querying data using dbplyr is fantastic, but R does a lot of magic behind the scenes in the form of type changing. Enums and uuids both get changed to strings. And you don‚Äôt need to do anything with foreign keys to link your tables together‚Äìyou just know which columns you want to join by and join them. This resulted in the schema visualizer showing each table in a vertical line with no relationships between them. It also was incorrectly forgiving when saving values that had inconsistent capitalization or other subtle differences.\n\n\nCI/CD\nFor some of my earlier projects, my deployments involved building a docker image, pushing it to Docker Hub, then SSHing into my ec2 instance on AWS, and then pulling and running the container. For Harvvest, we switched to GCP and I used Cloud Run and Cloud Build. Both of these made it very easy for me to build and deploy just by merging a pull request on Github. I had encountered a few cases where my local testing using docker compose would behave differently than the cloud deployment. Sometimes it was related to resources available to the container. Several times, I forgot to add new environment variables. One of the most frustrating was that Cloud Run had a harder time executing a complex initial command, such as importing my libraries and starting Plumber. For that I created an init.R file with all the setup commands an then used a simple Rscript init.R in my Dockerfile.\nWith my setup, building and deploying from Github was the only thing I knew how to do. A downside to this was that I was building separate containers for dev and production. For our new pipeline we are going to switch to only building the dev app, and then re-deploying that image in production when testing is complete. Additionally, we have done very little with testing via Github actions and are planning to incorporate more of that to help us feel more confident in our releases."
  },
  {
    "objectID": "posts/2026-02-14-deploying-health-tech/index.html",
    "href": "posts/2026-02-14-deploying-health-tech/index.html",
    "title": "Deploying Innovation: Making Healthcare Technology Billable and Integrated",
    "section": "",
    "text": "Integrating cutting-edge medical technology into existing healthcare practices presents a dual challenge: ensuring its clinical efficacy and making it a viable, billable service. Success hinges not just on the brilliance of the innovation, but on its seamless integration into the day-to-day operations and financial structures of medical facilities.\nI‚Äôve recently consulted with an EEG company that has developed technology with impressive diagnostic promise that expanded applications beyond seizures and into detection of various psychiatric pathologies. A challenge here is that psychiatrists or primary care physicians who would benefit from using the technology do not receive much training in EEG interpretation.\n\n\n\nThe squiggles of an EEG\n\n\nIn today‚Äôs AI revolution, it‚Äôs natural to say, ‚ÄúAI can do the interpretation on behalf of physicians who can‚Äôt do the interpretation themselves.‚Äù This approach will probably become viable within the next 5-10 years, but for now, it has a fatal flaw: the billing codes associated with EEG procedures require human interpretation.\nIn the current fee-for-service model, clinical processes must fit into CPT codes that can then be negotiated and reimbursed by payers. Here are some of the considerations I think about when developing a deployment strategy for a new health technology.\n\nThe Stakeholder Landscape\nA truly effective technology deployment considers the diverse needs of all involved parties:\n\nPatients: Seeking accurate diagnoses, effective treatments, and a smooth, reassuring experience.\nProviders: Focused on patient care, efficient workflows, and reliable tools that enhance their diagnostic and therapeutic capabilities.\nTechnicians and Staff: Requiring user-friendly equipment and clear protocols that enable them to administer tests effectively and consistently.\nAdministrators: Concerned with return on investment, operational efficiency, compliance, and ensuring the financial health of the practice through successful billing and reimbursement.\n\nAddressing these varied perspectives is crucial for widespread adoption and sustained growth.\n\n\nKey Pillars for Successful Technology Deployment\nSuccessful health tech integration extends beyond the core product to encompass the entire operational ecosystem.\n\nEase of Use for Technicians & Providers\nThe initial touchpoint with any new technology is often its usability. Hardware that ‚Äújust works‚Äù and is perceived as reliable and easy to operate by technicians forms a strong foundation. Similarly, intuitive software interfaces that align with a provider‚Äôs existing ‚Äúmuscle memory‚Äù reduce the learning curve and foster quicker adoption. Reports generated by the technology must not only be accurate but also specifically useful, integrating smoothly into clinical decision-making processes and supporting diagnostic workflows.\n\n\nThe Crucial Role of Billing Support\nNow that we have a solution that can be administered easily and has clinical benefit, we need to fit this into a billable clinical workflow. For EEG, two important codes are 95812 and 95816. These codes cover different durations of EEG recordings. The important shared components are that it covers both the administration and the interpretation of the test and that AI interpretation alone is explicitly not reimbursible. Because a provider spending more time on interpretation does not allow them to upcode and increase reimbursement, clinical review must be a streamlined, efficient process. Otherwise, provider time, one of the most expensive assets of a practice, is squandered on a fixed revenue task.\nFor this client, I recommended a user experience that allows a provider to view an AI analysis and the raw recording side-by-side with the ability to annotate and make notes. This way, a final report can be produced which includes the length of the recording as well as the manual assessment of the provider, completing the billable encounter.\n\n\n\nConclusion\nUltimately, the successful deployment of new healthcare technology is a holistic endeavor. It requires meticulous attention to the clinical workflow, user experience, and, critically, the financial viability of the service it enables. By deeply integrating into practice operations and actively supporting the creation of billable services, health technology can truly transform patient care and foster sustainable growth."
  }
]